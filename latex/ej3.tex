\section{Ejercicio 3}
    % 1. Describir detalladamente el problema a resolver dando ejemplos del mismo y sus soluciones.
    \subsection{Descripción del problema}

		\begin{figure}[ht]
			\begin{center}
				\includegraphics[width=0.4\columnwidth]{imagenes/tesoros.jpg}
				\caption{Fortuna y gloria muñeca, fortuna y gloria}
			\end{center}
		\end{figure}

        Una vez equilibrada la balanza, las paredes vuelven a su lugar. Con la llave obtenida logran abrir la puerta que estaba trabada. \par
		Al iluminar la habitación se encuentran con una sala de N tesoros. \par
		Por cada tipo de tesoro i, hay $C_{i}$ cantidades, con un valor $V_{i}$ y un peso $P_{i}$. Si bien el objetivo principal de la expedición era otro, nunca viene mal armarse de algún recuerdo. 
		Tienen M mochilas disponibles para llevarse los tesoros y cada una tiene una capacidad máxima de peso $K_{j}$ que puede llevar. \par
		El objetivo es llevarse el mayor valor posible de tesoros. Para esto, se debe devolver como resultado el valor total de tesoros guardados y además, por cada mochila, la cantidad de tesoros llevados y de qué tipo son.

        Por ejemplo, para la siguiente entrada:
        
        \begin{verbatim}
        M = 2
        K = { 1, 3 }
        
        N = 3 
        C = { 1, 2, 3 }
        V = { 2, 4, 10 }
        P = { 1, 2, 5 }
        \end{verbatim}

        Una posible salida válida sería:

        \begin{verbatim}
        S = 6
        M1 = { 1, 1 } 
        M2 = { 1, 2 }
        \end{verbatim}

        Es decir, para el caso en el que tenemos dos mochilas con capacidades 1 y 3, y nos encontramos con tres tesoros, el primero de peso 1 y valor 2, el segundo de peso 2 y valor 4, y el tercero de peso 5 y valor 10... \par Nos llevaremos en la primer mochila el primer tesoro y en la segunda el segundo tesoro. El tercer tesoro ya no nos entra.

    % 2. Explicar de forma clara, sencilla, estructurada y concisa, las ideas desarrolladas para la resolución del problema. Utilizar pseudocódigo y lenguaje coloquial (no código fuente). Justificar por qué el procedimiento resuelve efectivamente el problema.
    \subsection{Solución propuesta}
    Para solucionar el problema planteado se usó la técnica algorítmica de \emph{Programación dinámica}. \par
    La idea es separar el problema principal en subproblemas. En este caso, el subproblema es, en base a la capacidad restante de cada mochila y al valor y peso de cada tesoro, en qué mochila conviene meter cada uno (incluyendo la opción de no meterlo en ninguna). \par
    Por lo tanto, el algoritmo propuesto consta de una matriz para cada tesoro, de tamaño $(N+1)*(M+1)$ (suponiendo que hay dos mochilas), siendo éstas las capacidades de las mochilas $M_{1}$, $M_{2}$ respectivamente. Y además una matriz nula de igual dimensión que servirá para el caso en donde no hayan tesoros. \par
    El algoritmo funciona de la siguiente manera:
    1) Se crea una matriz de $M_{1}$ * $M_{2}$ para cada tesoro, y una extra inicial con los valores de todas las posiciones inicializados en 0.

    \begin{codesnippet}
    \begin{verbatim}
    for i = 1 .. cantTesoros + 1
        for j = 1 .. capacidadMochila1 + 1
            for k = 1 .. capacidadMochila2 + 1
                matriz[i][j][k] <-- 0
            end
        end
    end
    \end{verbatim}
    \end{codesnippet}

    2) Para cada posición (i, j) de la matriz del tesoro t, siendo i y j la capacidad \emph{restante} de la mochila 1 y 2, se pregunta si t entra en alguna de las dos mochilas.
    Si entra en la mochila 1, se obtiene valor(t) sumado al valor de la matriz del tesoro t-1 (recordar que el primer tesoro tiene una matriz predecesora con todos sus valores nulos) en la posición correspondiente según la capacidad restante de esa mochila (i - peso(t), j).
    Se repite este procedimiento para cada mochila. 
    Y además se obtiene el valor de la posición (i, j) de la matriz del tesoro t-1 para incluir el caso en el que no se mete el objeto en la mochila.
    Luego se obtiene el resultado mayor de cada suma, y se guarda en la matriz del tesoro t, en la posición (i, j)..

    \begin{codesnippet}
    \begin{verbatim}

    valorMochila1 = 0
    valorMochila2 = 0
    valorNingunaMochila = matriz[t-1][i][j]

    if pesoTesoro <= i then
        valorMochila1 <-- valor(t) + matriz[t-1][i - pesoTesoro][j]
    endif

    if pesoTesoro <= j then
        valorMochila2 <-- valor(t) + matriz[t-1][i][j - pesoTesoro]
    endif

    matriz[t][i][j] <-- max(valorMochila1, valorMochila2)

    \end{verbatim}
    \end{codesnippet}

    Se repite este procedimiento para cada tesoro, hasta completar todas las posiciones de cada matriz.
    El último valor obtenido de la matriz del último tesoro, será el valor máximo que se puede acumular en las mochilas con los tesoros.

    3) Luego, empezando desde la matriz del último tesoro, desde la última posición (i, j), se pregunta si entra ese tesoro en alguna mochila. 
    Si no entra, se repite el procedimiento para el tesoro anterior.
    Si entra, se pregunta en cuáles de las mochilas entra ese tesoro. Para eso se guarda la capacidad disponible de cada mochila (inicialmente con su capacidad total) y se pregunta si el tesoro tiene peso menor o igual a cada capacidad.
    En caso de que entre en más de una mochila, se compara el valor resultante de ponerlo en cada una. Por ejemplo, para la mochila 1, se obtiene el valor de la posición (i - peso(t), j) de la matriz del tesoro t-1.
    Una vez obtenido en qué mochila es conveniente poner a ese tesoro, se almacena en cada vector de salida (hay uno por mochila) los tipos de tesoro que guardo en cada mochila. Se incrementa el contador de tesoros de esa mochila, y luego el indice pasa a ser aquella posición donde estaba el valor máximo. Por ejemplo, si la mejor opción es guardar ese tesoro en la mochila 1, (i, j) pasa a ser (i - peso(t), j), t pasa a ser t-1, y se le resta peso(t) a la capacidad restante de esa mochila.

    \begin{codesnippet}
    \begin{verbatim}

    entraEnAlgunaMochila <-- matriz[t+1][i][j] != matriz[t][i][j]

    if entraEnAlgunaMochila then
    	valorAnterior1 <-- 0
    	valorAnterior2 <-- 0
	    entraEnMochila1 <-- (pesoTesoro <= i);
		entraEnMochila2 <-- (pesoTesoro <= j);

		if entraEnMochila1 then
			valorAnterior1 <-- matriz[t][i - pesoTesoro][j]
		endif
		if entraEnMochila2 then
			valorAnterior2 <-- matriz[t][i][j - pesoTesoro]
		endif

		if entraEnMochila1 && valorAnterior1 >= valorAnterior2 then
			add(mochila1, tipo(t))
			cantTesorosMochila1++
			i -= peso(t)
		else
			add(mochila2, tipo(t))
			cantTesorosMochila2++
			j -= peso(t)
		endif

	endif

    \end{verbatim}
    \end{codesnippet}

    Se repite este procedimiento para cada tesoro.

    4) Una vez finalizados los primeros tres pasos, se copia en una tupla el valor obtenido entre todos los tesoros guardados en las mochilas, y un vector mochilas que contiene un vector por mochila Cada uno de estos vectores guarda la cantidad de tesoros y los tipos de tesoro que hay en esa mochila.

    Para incluir una tercera mochila, se extendió la matriz a 3 dimensiones (4 para ser correctos, ya que una primera dimensión es para agrupar en un mismo vector la matriz de cada tesoro). Y cada vez que se realiza una comparación de valores máximos, se compara también con los valores de la tercer mochila.



    JULI:
    El objetivo de hacer esto, es tener en cuenta todos los casos y valores acumulados, de poner o no cada tesoro, y en qué mochila se pone. Utilizando en todo caso el mayor valor de tesoros acumulados. En los campos de la matriz ira el valor que queda de poner el tesoro o no, y las capacidades restantes de las mochilas. \par
    El algoritmo se divide en dos partes, una parte es completar las matrices de tesoros, con el maximo valor posible que se puede acumular en cada mochila según el peso disponible. La segunda parte, consiste en definir, qué tesoros van en cada mochila, ubicando según el valor restante, de poner el tesoro en una mochila u otra.

    Para esto, uno puede pensarlo como una matriz de tres dimensiones o un cubo por tesoro, además de un cubo nulo. Por cada tesoro que se analiza, los pasos son iguales. Si la capacidad de las mochilas, representado por el índice (i,j,k) que las recorre (i representa la capacidad de la M1, j de la M2 y K de la M3) es menor que el peso del tesoro, el valor obtenido es igual al acumulado en el tesoro anterior (salvo el primer caso, en donde el tesoro anterior es la matriz nula). Si entra en alguna mochila, uno debe fijarse, que valor restante se obtiene, de restarle a su índice, el peso del objeto. Y, en la posición de la matriz se deja el máximo de ponerlo en alguna mochila o no ponerlo.\par

    Una vez explicado brevemente, pasaremos a detallar el algoritmo, dividiéndolo en dos secciones. La primera, completa las matrices de los tesoros, según el valor máximo posible por capacidad. Mientras que la segunda sección, recorre de atras para adelante las matrices, para ubicar los tesoros.\par

    El siguiente pseudocodigo mostrara como se completan las matrices,

    \begin{codesnippet}
	\begin{verbatim}
    cuboMagico = Inicializo un vector de matrices de Int
    Inicializo cantTesoros+1 matrices de (N + 1)*(M + 1)*(K +1) y las guardo en el vector cuboMagico.
    //N, M y K capacidades de las mochilas.
    for(iTesoroActual= 0...cuboMagico.size()) do
        //Recorro todas las matrices de tesoros
        iTesoro = El indice a la matriz del tesoro anterior.
        valorTesoro = tesoroValor[iTesoroActual]
        pesoTesoro = tesoroPeso[iTesoroActual]
        valorNingunaMochila = 0; //Me guardo el valor por si no entra en ninguna mochila
        //Recorro todas las matrices de tesoros
        for(iMochila1 = 0 .. N + 1) do
            for(iMochila2 = 0 .. M + 1) do
                for(iMochila3 = 0 .. N + 1) do
                    if(pesoTesoro entra en alguna mochila) do
                        if(pesoTesoro entra en mochila M_{1})
                            valorMochila1 = valorTesoro + cuboMagico[iTesoro][iMochila1-pesoTesoro][iMochila2][iMochila3]; 
                            //Valor de la matriz del tesoro anterior.
                        endif
                        if(pesoTesoro entra en mochila M_{2}) do
                    	    valorMochila2 = valorTesoro +  cuboMagico[iTesoro][iMochila1][iMochila2 - pesoTesoro][iMochila3];
                    	    //Valor de la matriz del tesoro anterior.
                        endif
                        if(pesoTesoro entra en mochila M_{3}) do
                            valorMochila3 = valorTesoro + cuboMagico[iTesoro][iMochila1][iMochila2][iMochila3 - pesoTesoro];
                            //Valor de la matriz del tesoro anterior.
                        endif
                    cuboMagico[iTesoro][iMochila1][iMochila2][iMochila3] = max(valorNingunaMochila, valorMochila1, valorMochila2, valorMochila3);
                    //Me quedo con el maximo de ponerlo en alguna mochila o no ponerlo.
                    iTesoro ++ //Avanzo el indice al tesoro anterior.
                end
            end
        end

	\end{verbatim}
	\end{codesnippet}






	Para la segunda parte, se decide por cada tesoro, en qué mochila irá. Para esto, se recorre desde la última posición de la matriz del último tesoro. En esta posición, se encuentra el mejor valor acumulado posible. La decisión se toma de la siguiente manera
		%Listita:
		 A) En qué mochilas entra?
		 B) Para cada mochila en la cual entra. Qué valor restante se encuentra en la posición de la matriz del del tesoro anterior en los pesos restantes, de ponerlo en cada una de las mochilas.
		 C) En el caso que el valor restante sea igual, lo pongo en cualquiera de las dos
		 D) En el caso que haya un máximo, ubico el tesoro en la mochila que obtiene el valor máximo restante.
	Sigo este algoritmo por cada matriz, hasta no tener capacidad disponible en ninguna de las mochilas.

	Si se analiza la forma en que se construye la solución, tanto en la primera como en la segunda etapa, se tienen en cuenta cuál es el valor máximo posible acumulado, de ubicar los tesoros. Es por esto que el resultado, siempre dará la suma máxima posible.
	FIN JULI

   

    \subsubsection{Detalles implementativos}
    
    El algoritmo fue implementado en lenguaje C++. Para almacenar la solución, se recurre a la clase \texttt{vector}, proporcionada por la librería estándar del lenguaje.

    Durante la ejecución del algoritmo se utilizan dos variables globales; un entero para almacenar la cantidad de enemigos y un vector que contiene la ubicación de cada uno de ellos. La ejecución consiste principalmente de una función recursiva, que presenta las siguientes características:

    \begin{itemize}
        \item Se prueban todos los pares de enemigos posibles mediante dos ciclos anidados, de la forma que se describe en el primer punto de la sección anterior.
        \item Dentro del ciclo interior se revisa a qué enemigos destruye el kamehameha descripto por el par de enemigos elegido y se destruyen esos enemigos para luego agregarlos a la solución.
        \item Destruir los enemigos consiste en eliminarlos de un vector que contiene a los enemigos restantes. Este vector es una copia del vector de enemigos restantes; el original debe conservarse porque, para cada kamehameha distinto, el algoritmo se bifurca con diferentes enemigos restantes.
        \item Agregarlos a la solución consiste en añadir un vector que contiene a dichos enemigos a un vector de vectores, que representa la solución parcial construida hasta el momento: cada posición del vector corresponde a un kamehameha distinto y contiene a los enemigos que el mismo destruye.
        \item Sus casos base son las llamadas donde la cantidad de enemigos restantes es igual o menor que 2. Estos casos no necesitan realizar llamadas recursivas, y simplemente devuelven un vector cuyo único enemigo es el vector de los enemigos a destruir, ya que estos pueden ser liquidados con un único kamehameha.
    \end{itemize}

    % hecho. Deducir una cota de complejidad temporal del algoritmo propuesto y justificar por qué el algoritmo cumple la cota dada. Utilizar el modelo uniforme.
    \subsection{Complejidad teórica}

    Al comienzo se inicializa un vector de $N$ posiciones, lo cual tiene costo del orden de $N$. El algoritmo consiste principalmente de una función recursiva, cuyo costo determina la complejidad temporal total de la solución.
    
    Dicha función contiene dos ciclos, un dentro del otro; el ciclo exterior recorre índices desde $0$ hasta $N - 1$, y el interior, desde $i$ hasta $N$. Esto implica que lo implementado dentro del ciclo interior se ejecute $N \times (N - 1)$ veces. Analizando este último ciclo, puede verse que:

    \begin{itemize}
        \item Se recorren todos los kamehamehas anteriormente lanzados, que a lo sumo pueden ser $N/2$, ya que cada kamehameha elimina al menos a 2 androides. Esto tiene un costo del orden de $N$.
        \item Se destruyen los enemigos alcanzados por el kamehameha. Esto consiste en recorrerlos todos (ciclo con orden de $N$ iteraciones) y borrar cada uno del vector donde estan almacenados (esto cuesta también $\ord(N)$), con un costo del orden de $N^2$. Luego se los agrega a un vector con enemigos destruidos, con un costo del orden de $N$. Por lo tanto, el costo total es del orden de $N^2$.
        \item Por último se llama a la misma función en forma recursiva.
    \end{itemize}

    Se debe observar que al hacer el nuevo llamado, la cantidad de enemigos va a haber disminuido al menos en 2. Debido a esto, dentro de la siguiente llamada, el ciclo exterior recorrerá solo $N - 2$ indices y el interior $N - 3$. Así estas cantidades disminuirán de a 2 hasta terminar, llegando a 1.

    %% IMPORTANTE: LLEGUÉ A REVISAR HASTA ACÁ.

    Si pensamos la cantidad de llamadas recursivas como un árbol donde cada node es una llamada, tenemos un árbol de a lo sumo $N/2$ niveles, donde en su primer nivel hay $N \times (N - 1)$ llamados. Cada uno tiene $(N - 2) \times (N - 3)$ hijos, a su vez cada uno de ellos $(N - 4) \times (N - 5)$ y así sucesivamente hasta llegar a 1. Por lo que en el segundo nivel habrán $N \times (N - 1) \times (N - 2) \times (N - 3)$ nodos, y de esta forma en el último nivel se encontraran $N!$ hojas.

    El árbol no puede tener más de $N/2$ niveles ya que cada nivel representa un kamehameha, y no pueden haber más de $N/2$ kamehamehas.

    Ejemplo para $N$ = 4: 

    \begin{center}
    \includegraphics[width=0.6\columnwidth]{imagenes/ej3_arbol.pdf}
    \end{center}
    
    Se puede ver como para el caso de $N = 4$ tenemos que la raiz tiene $4 \times 3 = 12$ hijos y que cada uno tiene $2 \times 1 = 2$ hijos. En otras palabras, en la primera ejecución (la raíz) se hacen 12 llamados (uno por cada par de enemigos), y para cada uno se hacen dos llamados (uno por cada par de enemigos restantes).

    En el nivel $k$ con $0 \leq k \leq N / 2$ se puede ver que cada nodo tiene $(N - 2k) \times (N - 2k - 1)$ hijos,
    debido a los ciclos explicados previamente. Entonces la cantidad de nodos en el nivel $k$ está dada por la cantidad de nodos en $k - 1$ multiplcado por $(N - 2k) \times (N - 2k -1)$. De esta forma la cantidad de nodos en el nivel $k$ es igual al producto de las cantidades de hijos por nodo de cada nivel, hasta $k$. Es decir, 

    \[\text{\#nodos en nivel }k = \prod_{l = 0}^{k}(N - 2l) \times (N - 2l -1)\]

    Y por lo tanto se cumple para el último nivel ($k = N/2$) donde los nodos son hojas

    \[\text{\#hojas }= \prod_{l = 0}^{N / 2}(N - 2l) \times (N - 2l -1) = N!\]

    Sabiendo que la cantidad de hojas es $N!$ y la altura es $N/2$, y que cada llamada tiene un costo de orden $N^2$, se puede pensar que este arbol de ejecución está acotado superiormente por una ``matriz'' de $N/2$ filas y $N!$ columnas donde cada elemento es una llamada de costo $N^2$.

    Es decir, la complejidad del algoritmo está acotada superiormente por $N/2 \times N! \times N^2$ que es de orden $N^3 \times N!$.

    Ahora veremos que vale $N^k \times N! \leq k! \times N^N$ $\forall k$ constante, particularmente lo vale para $k = 3$, y que por lo tanto $N^3 \times N!$ está acotado superiormente por $N^N$ que a su vez está acotado por $N^{N + 2}$, por lo que el algortimo cumple la complejidad requerida.

    \[ N^N = \prod_{i = 0}^{n - 1}n = N^k \times \prod_{i = 0}^{N - k - 1} N \geq N^k \times \prod_{i = 0}^{N - k - 1} (N - 1) = N^k \times \frac{N!}{k!}\]

    Dado que $N^N \geq N^k \times \frac{N!}{k!} \implies k! \times N^N \geq N^k \times N!$ y por lo tanto $N^k \times N! \in \ord(N^N) \forall k \in \nat$

    % 4. Dar un código fuente claro que implemente la solución propuesta. Se deben incluir las partes relevantes del código como apéndice del informe impreso entregado.

    % 5. Realizar una experimentación computacional para medir la performance del programa implementado. Usar un conjunto de casos de test en función de los parámetros de entrada, con instancias aleatorias e instancias particulares (de peor/mejor caso en tiempo de ejecución, por ejemplo). Presentar en forma gráfica una comparación entre los tiempos medidos y la complejidad teórica calculada y extraer conclusiones.
    \subsection{Experimentación}

        Al igual que con los otros dos ejercicios, se realizaron pruebas experimentales para verificar que el tiempo de ejecución del algoritmo cumpliera con la cota asintótica de $\ord(N^3 \times N!)$, teóricamente demostrada para el peor caso. Se realizaron dos tipos de pruebas:
        
        \begin{itemize}
            \item Pruebas con instancias con características particulares, más específicamente, para el mejor caso, el peor caso y casos intermedios.
            \item Pruebas con instancias generadas aleatoriamente, para obtener una aproximación al comportamiento del algoritmo en el caso promedio.
        \end{itemize}

        \subsubsection{Instancias particulares}

            Todas las instancias utilizadas para estas pruebas se generaron de manera aleatoria, pero restringiendo los resultados obtenidos para cumplir con determinadas características. A continuación se enumeran los criterios tenidos en cuenta para la generación de los escenarios de prueba.

            \begin{itemize}
                \item \textbf{Mejor caso:} El mejor caso del algoritmo se produce cuando todos los androides a destruir están ubicados sobre una única línea recta. En este caso, es seguro que el primer kamehameha con el que se intente será la solución óptima del problema, por lo que solo se bajará por esa rama de la recursión.

                \item \textbf{Peor caso:} El peor caso del algoritmo se da cuando no existen tres androides alineados a destruir. En consecuencia, cada kamehameha destruirá tan solo dos enemigos, que es el mínimo posible. Esto maximiza la complejidad temporal del algoritmo por dos motivos. Por un lado, en cada nivel de la recursión, se intentarán disparar todos los kamehamehas posibles, con la esperanza de que pueda encontrarse una solución más razonable. Por otra parte, en cada llamada recursiva que se efectúe, la entrada solo se reducirá en dos elementos, haciendo que la profundidad de la recursión alcance siempre la cota máxima de $\frac{N}{2}$.

                \item \textbf{Caso intermedio:} También se agregó un escenario de prueba adicional, con el objetivo principal de poner a prueba la efectividad de las podas implementadas. El mismo consiste en la combinación de dos instancias de mejor caso de tamaño $\frac{N}{2}$; es decir, la mitad de los androides se encuentran sobre una línea recta y la otra mitad, sobre una recta diferente. Las coordenadas de los enemigos son mezcladas de forma aleatoria para evitar que el kamehameha óptimo sea necesariamente el primero que se intente.
                
                Bajo estas condiciones, cabe esperar que las podas entren en acción, reduciendo considerablemente cantidad de llamadas recursivas que se ejecutan completas y consiguiendo un rendimiento apreciablemente superior al observado en el peor caso.

                Cabe destacar que este es un simple caso adicional de prueba, concebido con el objetivo de ilustrar la gran variación en el rendimiento del algoritmo según las características de los datos de entrada, pero que no guarda relación alguna con el comportamiento del algoritmo en el caso promedio.
            \end{itemize}

            Para los tres escenarios previstos, se generaron instancias de prueba para todos los valores de $N$ entre $1$ y $12$, inclusive. Luego, en cada caso, se ejecutaron $60$ repeticiones del algoritmo, midiendo cada vez el tiempo de ejecución y tomando luego el promedio entre los resultados obtenidos.

            En un principio, se descubrió que se producían picos en el tiempo de ejecución en las primeras corridas del programa, tendiendo los valores a estabilizarse con las sucesivas corridas. Si bien no se pudo determinar con precisión el origen de estas anomalías, se decidió, para minimizar la varianza de los datos obtenidos, tratar a las primeras $20$ repeticiones de cada instancia como \emph{outliers}, y promediar solo los valores de las últimas $40$.

            \renewcommand\constante{0.1}

            Los resultados obtenidos se exponen en el gráfico de la Figura \ref{fig:exp3:part_tiempo_base}, donde se ilustra también la cota teórica de $c \times N^3 \times N!$ (el valor de $c$ utilizado es \constante). Se representan como $T_P$ los tiempos obtenidos para peor caso, como $T_M$ para el mejor caso, y como $T_I$ para el caso intermedio. Puede observarse claramente que, incluso en el peor escenario, el tiempo de ejecución del algoritmo tiende a aumentar a un ritmo estrictamente más lento que el de la cota prevista.

            \begin{figure}[H]
                \centering
                \caption{}
                \label{fig:exp3:part_tiempo_base}
                \begin{tikzpicture}
                    \begin{axis}[
                            title={},
                            xlabel={Tamaño de entrada ($N$)},
                            ylabel={Tiempo de ejecución (nanosegundos)},
                            ymode = log,
                            scaled x ticks=false,
                            scaled y ticks=false,
                            enlargelimits=0.05,
                            width=0.5\textwidth,
                            height=0.5\textwidth,
                            legend pos=north west,
                            legend cell align=left,
                            xmin=1
                        ]

                        \addplot[color=red] table[x index=0,y index=1]{../exp/kamehamehaPeor};
                        \addplot[color=blue] table[x index=0,y index=1]{../exp/kamehamehaIntermedio};
                        \addplot[color=green] table[x index=0,y index=1]{../exp/kamehamehaMejor};
                        \addplot[color=gray] table[x index=0, y expr={\constante * (x^3) * x!}]{../exp/kamehamehaPeor};
                        \legend{$T_P(N)$, $T_M(N)$, $T_I(N)$, $c \times N^3 \times N!$}
                    \end{axis}
                \end{tikzpicture}
            \end{figure}

            En la Figura \ref{fig:exp3:part_tiempo_sobre_exp} se muestra el cociente entre los datos obtenidos y la función $N^3 \times N!$ (se considera el mismo valor de $c$ que en el gráfico anterior). Puede observarse como, al aumentar el tamaño de $N$, el cociente se aproxima rápidamente a $0$, ilustrando el hecho de que el crecimiento asintótico de la cota es estrictamente mayor que el de los tiempos obtenidos en las mediciones.

            \begin{figure}[H]
                \centering
                \caption{}
                \label{fig:exp3:part_tiempo_sobre_exp}
                \begin{tikzpicture}
                    \begin{axis}[
                            title={},
                            xlabel={Tamaño de entrada ($N$)},
                            ylabel={Tiempo de ejecución (nanosegundos)},
                            ymode = log,
                            scaled x ticks=false,
                            scaled y ticks=false,
                            enlargelimits=0.05,
                            width=0.5\textwidth,
                            height=0.5\textwidth,
                            legend pos=north west,
                            legend cell align=left,
                            xmin=1
                        ]

                        \addplot[color=red] table[x index=0,y expr={\thisrowno{1} / (x^(x+2))}]{../exp/kamehamehaPeor};
                        \addplot[color=blue] table[x index=0,y expr={\thisrowno{1} / (x^(x+2))}]{../exp/kamehamehaIntermedio};
                        \addplot[color=green] table[x index=0,y expr={\thisrowno{1} / (x^(x+2))}]{../exp/kamehamehaMejor};
                        \addplot[color=gray] table[x index=0, y expr={\constante}]{../exp/kamehamehaPeor};
                        \legend{$\frac{T_P(N)}{N^3 \times N!}$, $\frac{T_M(N)}{N^3 \times N!}$, $\frac{T_I(N)}{N^3 \times N!}$, $c$}
                    \end{axis}
                \end{tikzpicture}
            \end{figure}

            El análisis expuesto de los datos recopilados presenta evidencia empírica sobre la pertinencia la cota de complejidad demostrada teóricamente. Más aún, permite llegar a la conclusión que, incluso en las instancias de peor caso, esta cota resulta holgada, es decir, que la complejidad del algoritmo presentado es estrictamente $\ord(N^3 \times N!)$.

        \subsubsection{Instancias aleatorias}

            Para obtener una aproximación al comportamiento del algoritmo en el caso promedio, se realizaron también pruebas con instancias generadas de forma completamente aleatoria. Este experimento también se realizó para los valores de $N$ entre $1$ y $12$, y al igual que el anterior, la prueba se repitió $60$ veces para cada valor de $N$, descartando los resultados de las primeras $20$ repeticiones y promediando los obtenidos en las otras $40$. Sin embargo, esta vez cada medición se realizó sobre una instancia de prueba diferente, generada al azar.

            Intuitivamente, resulta razonable conjeturar que la probabilidad de que al tres enemigos se encuentren alineados en una instancia aleatoria es muy baja, especialmente teniendo en cuenta los pequeños tamaños de entrada considerados. En otras palabras, parece esperable que un escenario aleatorio tenga características similares a un escenario de peor caso, y por lo tanto, que el rendimiento promedio del algoritmo sea similar a este último caso.

            En la figura \ref{fig:exp3:random_tiempo} se presentan los resultados obtenidos para estas instancias aleatorias ($T_R$), y se los compara con los previamente obtenidos para las instancias de peor caso ($T_P$). Como puede observarse, los datos empíricos respaldan la hipótesis recién expuesta. Cabe destacar que, a pesar de que todas las mediciones se realizaron sobre instancias diferentes, la varianza muestral observada fue comparable a la obtenida para el peor caso, donde las repeticiones se efectuaban con instancias idénticas, mostrando cierta uniformidad en el rendimiento del algoritmo cuando los datos de entrada no cumplen características particulares que permitan aprovechar las podas diseñadas.

            \begin{figure}[H]
                \centering
                \caption{}
                \label{fig:exp3:random_tiempo}
                \begin{tikzpicture}
                    \begin{axis}[
                            title={},
                            xlabel={Tamaño de entrada ($N$)},
                            ylabel={Tiempo de ejecución (nanosegundos)},
                            ymode = log,
                            scaled x ticks=false,
                            scaled y ticks=false,
                            enlargelimits=0.05,
                            width=0.5\textwidth,
                            height=0.5\textwidth,
                            legend pos=north west,
                            legend cell align=left,
                            xmin=1
                        ]
                        \addplot[color=red] table[x index=0, y index=1]{../exp/kamehamehaRandom};
                        \addplot[color=gray] table[x index=0, y index=1]{../exp/kamehamehaPeor};
                        \legend{$T_R(N)$, $T_P(N)$}
                    \end{axis}
                \end{tikzpicture}
            \end{figure}
